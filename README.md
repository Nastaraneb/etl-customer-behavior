# ğŸš€ Customer Behavior ETL Pipeline  

<img width="1911" height="816" alt="airflofinal" src="https://github.com/user-attachments/assets/1a66404d-8744-46cb-a371-b52f962e042f" />


*A Production-Ready ETL Pipeline Using Airflow, Docker, Python OOP, and Postgres*

This project implements a complete **ETL (Extractâ€“Transformâ€“Load) pipeline** to process customer behavior data from an e-commerce website.  
It uses **Apache Airflow** for workflow orchestration, **Docker Compose** for environment setup, **Python OOP** for preprocessing logic, and **Postgres** as the data warehouse.

The ETL pipeline is fully automated and containerized â€“ making it reproducible, scalable, and easy to deploy.

---

## ğŸ§  **Project Goals**

- Build a **real-world, production-style ETL pipeline**  
- Apply **data preprocessing** using clean OOP architecture  
- Automate workflows using **Airflow DAGs**  
- Load cleaned data into **Postgres**  
- Practice professional **Git branching & version control**  
- Create a portfolio-ready end-to-end data engineering project  

---

# ğŸ“Š **Pipeline Architecture**

The ETL pipeline consists of three main steps:

             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   Raw CSV Dataset    â”‚
             â”‚  (Customer Behavior) â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ Extract
                        â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Preprocessing Layer   â”‚
            â”‚ (Python OOP Class)     â”‚
            â”‚ - Clean data           â”‚
            â”‚ - Validate fields      â”‚
            â”‚ - Handle types/nulls   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ Transform
                       â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Processed Clean Dataset   â”‚
          â”‚  (.csv saved inside /data)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ Load
                      â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚        Postgres Database         â”‚
       â”‚  (customer_behavior table)       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

# âš™ï¸ **Tech Stack**

| Component | Technology |
|----------|------------|
| Workflow Orchestration | Apache Airflow 3.1 |
| Containerization | Docker & Docker Compose |
| Programming | Python 3.12 |
| Data Storage | Postgres (Custom Service) |
| Data Processing | Pandas |
| Architecture | OOP-based preprocessing class |
| Version Control | Git + GitHub Flow |
| Optional Dashboard | pgAdmin (optional) |

---

# ğŸ“ **Project Structure**
- `dags/customer_behavior_etl.py` â€“ Airflow DAG definition
- `processor/__init__.py` â€“ Package initializer
- `processor/preprocessing.py` â€“ Python OOP preprocessing module
- `data/raw/online_shoppers_intention.csv` â€“ Raw customer behavior dataset
- `data/processed/online_shoppers_preprocessed.csv` â€“ Cleaned / transformed dataset
- `docker-compose.yaml` â€“ Core Airflow services (scheduler, webserver, worker, triggerer, Redis, etc.)
- `docker-compose.override.yml` â€“ Custom Postgres service for the ETL pipeline
- `README.md` â€“ Project documentation

---

# ğŸ›  **How to Run the Project (Step-by-Step)**
1ï¸. Clone the Repository

git clone https://github.com/<your-username>/etl-customer-behavior.git
`cd etl-customer-behavior`

2ï¸. Start the Airflow + Postgres Environment
Make sure Docker Desktop is running.

docker-compose up -d
This command starts:
Airflow Scheduler
Airflow Webserver
Airflow Worker
Airflow Triggerer
Redis
Custom Postgres Database (etl_db)

3.Access Airflow UI
http://localhost:8080
Login credentials:

Username: airflow
Password: airflow
4ï¸. Trigger the ETL Pipeline

In Airflow:
Find the DAG: customer_behavior_etl
Turn it ON
Click Trigger DAG

5ï¸. Verify Load in Postgres

Enter the Postgres container:
docker exec -it etl_db psql -U etl_user -d customer_behavior
Query the table:
SELECT * FROM customer_behavior LIMIT 20;








